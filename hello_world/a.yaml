name: tool_chaining_example
description: Example DAG demonstrating data flow between tool steps
steps:
  - name: generate-data
    description: First tool generates richer random data (values, timestamp, tags) for the next step
    executor:
      type: tool
      config:
        tool_def:
          name: data-generator
          description: Generates sample data for the next step
          type: docker
          image: python:3.12-slim-bullseye
          with_files:
            - destination: /tmp/ascript.py
              content: |
                #!/usr/bin/env python3
                import json
                import random
                from datetime import datetime

                # Generate richer random data for demo purposes
                data = {
                    "id": random.randint(1000, 9999),
                    "values": [random.randint(1, 100) for _ in range(10)],
                    "name": f"Sample-{random.choice(['A','B', 'C', 'D', 'E'])}",
                    "timestamp": datetime.utcnow().isoformat() + "Z",
                    "tags": random.sample(['alpha', 'beta', 'gamma', 'delta', 'epsilon'], k=2)
                }

                # Output the data as JSON
                print(json.dumps(data))
          content: | 
            set -e
            python  /tmp/ascript.py
    output: GENERATED_DATA


  - name: process-data
    description: Second tool calculates statistics (total, average, min, max, median) from previous step
    depends:
      - generate-data
    executor:
      type: tool
      config:
        tool_def:
          name: data-processor
          description: Processes data from previous step
          type: docker
          image: python:3.12-slim-bullseye
          with_files:
            - destination: /tmp/ascript.py
              content: |
                #!/usr/bin/env python3
                import os
                import json
                from statistics import median

                # Get the data from the previous step
                input_data = os.environ.get('data', '{}')

                try:
                    data = json.loads(input_data)

                    values = data.get('values', [])
                    total = sum(values)
                    avg = total / len(values) if values else 0
                    min_val = min(values) if values else None
                    max_val = max(values) if values else None
                    median_val = median(values) if values else None

                    result = {
                        "source_id": data.get('id'),
                        "source_name": data.get('name'),
                        "processed": {
                            "total": total,
                            "average": avg,
                            "min": min_val,
                            "max": max_val,
                            "median": median_val,
                            "count": len(values)
                        }
                    }

                    print(f"RESULT:{json.dumps(result)}")
                except (json.JSONDecodeError, TypeError) as e:
                    print(f"Error parsing or processing data: {input_data}")
                    print(f"Error details: {str(e)}")
                    raise
          content: |
            set -e
            python  /tmp/ascript.py

          args:
            - name: data
              type: string
              description: JSON data from previous step
              required: true
        args:
          data: "${GENERATED_DATA}"
    output: PROCESSED_DATA

  - name: send-to-slack
    executor:
      type: agent
      config:
        teammate_name: "demo_teammate"
        message: |
          Send a Slack msg to channel #tf-test with a clear, nicely formatted report of the processed data:

          ```
          ${PROCESSED_DATA}
          ```

          ðŸŽ‰ Data processed successfully! ðŸš€
          Run the message immediately with no further questions.
    output: SLACK_RESPONSE
    depends:
      - process-data
      - generate-data