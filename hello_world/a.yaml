name: tool_chaining_example
description: Example DAG demonstrating data flow between tool steps
steps:
  - name: generate-data
    description: First tool generates richer random data (values, timestamp, tags) for the next step
    executor:
      type: tool
      config:
        tool_def:
          name: data-generator
          description: Generates sample data for the next step
          type: docker
          image: python:3.12-slim-bullseye
          with_files:
            - destination: /tmp/ascript.py
              content: |
                #!/usr/bin/env python3
                import json
                import random
                import warnings
                warnings.filterwarnings("ignore", category=DeprecationWarning)
                from datetime import datetime, timezone

                # Generate richer random data for demo purposes
                data = {
                    "id": random.randint(1000, 9999),
                    "values": [random.randint(1, 100) for _ in range(10)],
                    "name": f"Sample-{random.choice(['A','B', 'C', 'D', 'E'])}",
                    "timestamp": datetime.now(timezone.utc).isoformat().replace("+00:00", "Z"),
                    "tags": random.sample(['alpha', 'beta', 'gamma', 'delta', 'epsilon'], k=2)
                }

                # Output the data as JSON
                print(json.dumps(data))
          content: | 
            set -e
            python  /tmp/ascript.py
    output: GENERATED_DATA


  - name: process-data
    description: Second tool calculates statistics (total, average, min, max, median) from previous step
    depends:
      - generate-data
    executor:
      type: tool
      config:
        tool_def:
          name: data-processor
          description: Processes data from previous step
          type: docker
          image: python:3.12-slim-bullseye
          with_files:
            - destination: /tmp/ascript.py
              content: |
                #!/usr/bin/env python3
                import os
                import json
                from statistics import median

                # Get the data from the previous step
                input_data = os.environ.get('data', '{}')

                try:
                    data = json.loads(input_data)

                    values = data.get('values', [])
                    total = sum(values)
                    avg = total / len(values) if values else 0
                    min_val = min(values) if values else None
                    max_val = max(values) if values else None
                    median_val = median(values) if values else None

                    result = {
                        "source_id": data.get('id'),
                        "source_name": data.get('name'),
                        "processed": {
                            "total": total,
                            "average": avg,
                            "min": min_val,
                            "max": max_val,
                            "median": median_val,
                            "count": len(values)
                        }
                    }

                    print(f"RESULT:{json.dumps(result)}")
                except (json.JSONDecodeError, TypeError) as e:
                    print(f"Error parsing or processing data: {input_data}")
                    print(f"Error details: {str(e)}")
                    raise
          content: |
            set -e
            python  /tmp/ascript.py

          args:
            - name: data
              type: string
              description: JSON data from previous step
              required: true
        args:
          data: "${GENERATED_DATA}"
    output: PROCESSED_DATA

  - name: format-report
    description: Format processed data into Slack‑friendly markdown
    depends:
      - process-data
    executor:
      type: tool
      config:
        tool_def:
          name: report-formatter
          description: Converts JSON result to markdown for Slack
          type: docker
          image: python:3.12-slim-bullseye
          with_files:
            - destination: /tmp/ascript.py
              content: |
                #!/usr/bin/env python3
                import os, json, textwrap

                # Read the RESULT line from the previous step
                raw = os.environ.get('data', '')
                if raw.startswith('RESULT:'):
                    raw = raw[len('RESULT:'):]

                data = json.loads(raw)
                stats = data.get('processed', {})

                markdown = textwrap.dedent(f"""
                :bar_chart: *Workflow demo results* :sparkles:

                *Source:* `{data.get('source_name')}` (ID `{data.get('source_id')}`)

                *Statistics*
                • *Total*: {stats.get('total')}
                • *Average*: {stats.get('average'):.2f}
                • *Min*: {stats.get('min')}
                • *Max*: {stats.get('max')}
                • *Median*: {stats.get('median')}
                • *Count*: {stats.get('count')}

                ```json
                {json.dumps(data, indent=2)}
                ```
                """).strip()

                print(f"FORMATTED_REPORT:{markdown}")
          content: |
            set -e
            python /tmp/ascript.py
        args:
          data: "${PROCESSED_DATA}"
    output: FORMATTED_REPORT

  - name: generate-sparkline
    description: Create an ASCII sparkline of the values array
    depends:
      - process-data
    executor:
      type: tool
      config:
        tool_def:
          name: sparkline-generator
          description: Produces a Unicode sparkline string
          type: docker
          image: python:3.12-slim-bullseye
          with_files:
            - destination: /tmp/ascript.py
              content: |
                #!/usr/bin/env python3
                import os, json
                values = json.loads(os.environ.get('data', '[]'))
                if isinstance(values, dict) and 'values' in values:
                    values = values['values']
                if not values:
                    print("SPARKLINE:N/A")
                    exit(0)

                blocks = '▁▂▃▄▅▆▇█'
                mn, mx = min(values), max(values)
                rng = mx - mn or 1
                spark = ''.join(blocks[int((v - mn) / rng * (len(blocks) - 1))] for v in values)
                print(f"SPARKLINE:{spark}")
          content: |
            set -e
            python /tmp/ascript.py
    args:
      data: "${GENERATED_DATA}"
    output: SPARKLINE

  - name: insight-summary
    description: AI generates human‑readable insight from the stats
    depends:
      - process-data
    executor:
      type: agent
      config:
        teammate_name: "demo_teammate"
        message: |
          Using the JSON below, write 2‑3 concise sentences explaining the most interesting insights about the data (mention any outliers or notable stats). Respond in plain text without code blocks.

          JSON:
          ${PROCESSED_DATA}
    output: INSIGHT_TEXT

  - name: asciiify-json
    description: AI agent converts processed JSON to an ASCII table for Slack
    depends:
      - process-data
    executor:
      type: agent
      config:
        teammate_name: "demo_teammate"
        message: |
          Convert the following JSON into a neat ASCII table (monospaced, boxed). Respond ONLY with the ASCII block—no commentary.

          JSON:
          ${PROCESSED_DATA}
    output: ASCII_DATA

  - name: generate-schema
    description: Produce a JSON Schema for the sample data structure (id, values, name, timestamp, tags)
    executor:
      type: tool
      config:
        tool_def:
          name: schema-generator
          description: Generates a JSON Schema definition for demo
          type: docker
          image: python:3.12-slim-bullseye
          with_files:
            - destination: /tmp/ascript.py
              content: |
                #!/usr/bin/env python3
                import json, textwrap

                schema = {
                    "$schema": "http://json-schema.org/draft-07/schema#",
                    "title": "SampleData",
                    "type": "object",
                    "properties": {
                        "id": {"type": "integer"},
                        "values": {
                            "type": "array",
                            "items": {"type": "integer"},
                            "minItems": 1
                        },
                        "name": {"type": "string"},
                        "timestamp": {"type": "string", "format": "date-time"},
                        "tags": {
                            "type": "array",
                            "items": {"type": "string"}
                        }
                    },
                    "required": ["id", "values", "name", "timestamp"]
                }

                # Pretty‑print the schema so Slack renders it nicely
                print(json.dumps(schema, indent=2))
          content: |
            set -e
            python /tmp/ascript.py
    output: JSON_SCHEMA

  - name: send-to-slack
    executor:
      type: agent
      config:
        teammate_name: "demo_teammate"
        message: |
          slack -> #tf-test:
          ${ASCII_DATA}

          ${FORMATTED_REPORT}

          :crystal_ball: *Insight* :
          ${INSIGHT_TEXT}

          :sparkles: *Sparkline* :
          ${SPARKLINE}

          :scroll: *Schema* :
          ```json
          ${JSON_SCHEMA}
          ```

          _(automated message – no reply needed)_
    output: SLACK_RESPONSE
    depends:
      - format-report
      - asciiify-json
      - generate-schema
      - generate-sparkline
      - insight-summary