{"version":3,"sources":["../../../src/runtimes/edge/createEdgeRuntimeAPI.ts"],"sourcesContent":["import {\n  LanguageModelV1,\n  LanguageModelV1ToolChoice,\n  LanguageModelV1FunctionTool,\n  LanguageModelV1Prompt,\n  LanguageModelV1CallOptions,\n} from \"@ai-sdk/provider\";\nimport { CoreMessage, ThreadStep } from \"../../types/AssistantTypes\";\nimport { assistantEncoderStream } from \"./streams/assistantEncoderStream\";\nimport { EdgeRuntimeRequestOptionsSchema } from \"./EdgeRuntimeRequestOptions\";\nimport { toLanguageModelMessages } from \"./converters/toLanguageModelMessages\";\nimport { Tool } from \"../../types\";\nimport { toLanguageModelTools } from \"./converters/toLanguageModelTools\";\nimport {\n  toolResultStream,\n  ToolResultStreamPart,\n} from \"./streams/toolResultStream\";\nimport { runResultStream } from \"./streams/runResultStream\";\nimport {\n  LanguageModelConfig,\n  LanguageModelV1CallSettings,\n  LanguageModelV1CallSettingsSchema,\n} from \"../../types/ModelConfigTypes\";\nimport { CoreChatModelRunResult } from \"../local/ChatModelAdapter\";\nimport { streamPartEncoderStream } from \"./streams/utils/streamPartEncoderStream\";\nimport { z } from \"zod\";\n\ntype FinishResult = {\n  messages: CoreMessage[];\n  metadata: {\n    steps: ThreadStep[];\n  };\n};\n\ntype LanguageModelCreator = (\n  config: LanguageModelConfig,\n) => Promise<LanguageModelV1> | LanguageModelV1;\n\nexport type CreateEdgeRuntimeAPIOptions = LanguageModelV1CallSettings & {\n  model: LanguageModelV1 | LanguageModelCreator;\n  system?: string;\n  tools?: Record<string, Tool<any, any>>;\n  toolChoice?: LanguageModelV1ToolChoice;\n  onFinish?: (result: FinishResult) => void;\n};\n\nconst voidStream = () => {\n  return new WritableStream({\n    abort(reason) {\n      console.error(\"Server stream processing aborted:\", reason);\n    },\n  });\n};\n\ntype GetEdgeRuntimeStreamOptions = {\n  abortSignal: AbortSignal;\n  requestData: z.infer<typeof EdgeRuntimeRequestOptionsSchema>;\n  options: CreateEdgeRuntimeAPIOptions;\n};\n\nexport const getEdgeRuntimeStream = async ({\n  abortSignal,\n  requestData: unsafeRequest,\n  options: {\n    model: modelOrCreator,\n    system: serverSystem,\n    tools: serverTools = {},\n    toolChoice,\n    onFinish,\n    ...unsafeSettings\n  },\n}: GetEdgeRuntimeStreamOptions) => {\n  const settings = LanguageModelV1CallSettingsSchema.parse(unsafeSettings);\n  const lmServerTools = toLanguageModelTools(serverTools);\n  const hasServerTools = Object.values(serverTools).some((v) => !!v.execute);\n\n  const {\n    system: clientSystem,\n    tools: clientTools = [],\n    messages,\n    apiKey,\n    baseUrl,\n    modelName,\n    ...callSettings\n  } = EdgeRuntimeRequestOptionsSchema.parse(unsafeRequest);\n\n  const systemMessages = [];\n  if (serverSystem) systemMessages.push(serverSystem);\n  if (clientSystem) systemMessages.push(clientSystem);\n  const system = systemMessages.join(\"\\n\\n\");\n\n  for (const clientTool of clientTools) {\n    if (serverTools?.[clientTool.name]) {\n      throw new Error(\n        `Tool ${clientTool.name} was defined in both the client and server tools. This is not allowed.`,\n      );\n    }\n  }\n\n  const model =\n    typeof modelOrCreator === \"function\"\n      ? await modelOrCreator({ apiKey, baseUrl, modelName })\n      : modelOrCreator;\n\n  let stream: ReadableStream<ToolResultStreamPart>;\n  const streamResult = await streamMessage({\n    ...(settings as Partial<StreamMessageOptions>),\n    ...callSettings,\n\n    model,\n    abortSignal,\n\n    ...(!!system ? { system } : undefined),\n    messages: [...messages],\n    tools: lmServerTools.concat(clientTools as LanguageModelV1FunctionTool[]),\n    ...(toolChoice ? { toolChoice } : undefined),\n  });\n  stream = streamResult.stream;\n\n  // add tool results if we have server tools\n  const canExecuteTools = hasServerTools && toolChoice?.type !== \"none\";\n  if (canExecuteTools) {\n    stream = stream.pipeThrough(toolResultStream(serverTools, abortSignal));\n  }\n\n  if (canExecuteTools || onFinish) {\n    // tee the stream to process server tools and onFinish asap\n    const tees = stream.tee();\n    stream = tees[0];\n    let serverStream = tees[1];\n\n    if (onFinish) {\n      let lastChunk: CoreChatModelRunResult | undefined;\n      serverStream = serverStream.pipeThrough(runResultStream()).pipeThrough(\n        new TransformStream({\n          transform(chunk) {\n            lastChunk = chunk;\n          },\n          flush() {\n            if (!lastChunk?.status || lastChunk.status.type === \"running\")\n              return;\n\n            const resultingMessages = [\n              ...messages,\n              {\n                role: \"assistant\",\n                content: lastChunk.content,\n              } satisfies CoreMessage,\n            ];\n            onFinish({\n              messages: resultingMessages,\n              metadata: {\n                // TODO\n                // eslint-disable-next-line @typescript-eslint/no-non-null-asserted-optional-chain\n                steps: lastChunk.metadata?.steps!,\n              },\n            });\n          },\n        }),\n      );\n    }\n\n    // drain the server stream\n    serverStream.pipeTo(voidStream()).catch((e) => {\n      console.error(\"Server stream processing error:\", e);\n    });\n  }\n\n  return stream;\n};\n\nexport declare namespace getEdgeRuntimeResponse {\n  export type { GetEdgeRuntimeStreamOptions as Options };\n}\n\nexport const getEdgeRuntimeResponse = async (\n  options: getEdgeRuntimeResponse.Options,\n) => {\n  const stream = await getEdgeRuntimeStream(options);\n  return new Response(\n    stream\n      .pipeThrough(assistantEncoderStream())\n      .pipeThrough(streamPartEncoderStream()),\n    {\n      headers: {\n        \"Content-Type\": \"text/plain; charset=utf-8\",\n      },\n    },\n  );\n};\n\nexport const createEdgeRuntimeAPI = (options: CreateEdgeRuntimeAPIOptions) => ({\n  POST: async (request: Request) =>\n    getEdgeRuntimeResponse({\n      abortSignal: request.signal,\n      requestData: await request.json(),\n      options,\n    }),\n});\n\ntype StreamMessageOptions = LanguageModelV1CallSettings & {\n  model: LanguageModelV1;\n  system?: string;\n  messages: CoreMessage[];\n  tools?: LanguageModelV1FunctionTool[];\n  toolChoice?: LanguageModelV1ToolChoice;\n  abortSignal: AbortSignal;\n};\n\nasync function streamMessage({\n  model,\n  system,\n  messages,\n  tools,\n  toolChoice,\n  ...options\n}: StreamMessageOptions) {\n  return model.doStream({\n    inputFormat: \"messages\",\n    mode: {\n      type: \"regular\",\n      ...(tools ? { tools } : undefined),\n      ...(toolChoice ? { toolChoice } : undefined),\n    },\n    prompt: convertToLanguageModelPrompt(system, messages),\n    ...(options as Partial<LanguageModelV1CallOptions>),\n  });\n}\n\nexport function convertToLanguageModelPrompt(\n  system: string | undefined,\n  messages: CoreMessage[],\n): LanguageModelV1Prompt {\n  const languageModelMessages: LanguageModelV1Prompt = [];\n\n  if (system != null) {\n    languageModelMessages.push({ role: \"system\", content: system });\n  }\n  languageModelMessages.push(...toLanguageModelMessages(messages));\n\n  return languageModelMessages;\n}\n"],"mappings":";;;;;;;;;;;;;;;;;;;;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAQA,oCAAuC;AACvC,uCAAgD;AAChD,qCAAwC;AAExC,kCAAqC;AACrC,8BAGO;AACP,6BAAgC;AAChC,8BAIO;AAEP,qCAAwC;AAsBxC,IAAM,aAAa,MAAM;AACvB,SAAO,IAAI,eAAe;AAAA,IACxB,MAAM,QAAQ;AACZ,cAAQ,MAAM,qCAAqC,MAAM;AAAA,IAC3D;AAAA,EACF,CAAC;AACH;AAQO,IAAM,uBAAuB,OAAO;AAAA,EACzC;AAAA,EACA,aAAa;AAAA,EACb,SAAS;AAAA,IACP,OAAO;AAAA,IACP,QAAQ;AAAA,IACR,OAAO,cAAc,CAAC;AAAA,IACtB;AAAA,IACA;AAAA,IACA,GAAG;AAAA,EACL;AACF,MAAmC;AACjC,QAAM,WAAW,0DAAkC,MAAM,cAAc;AACvE,QAAM,oBAAgB,kDAAqB,WAAW;AACtD,QAAM,iBAAiB,OAAO,OAAO,WAAW,EAAE,KAAK,CAAC,MAAM,CAAC,CAAC,EAAE,OAAO;AAEzE,QAAM;AAAA,IACJ,QAAQ;AAAA,IACR,OAAO,cAAc,CAAC;AAAA,IACtB;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA,GAAG;AAAA,EACL,IAAI,iEAAgC,MAAM,aAAa;AAEvD,QAAM,iBAAiB,CAAC;AACxB,MAAI,aAAc,gBAAe,KAAK,YAAY;AAClD,MAAI,aAAc,gBAAe,KAAK,YAAY;AAClD,QAAM,SAAS,eAAe,KAAK,MAAM;AAEzC,aAAW,cAAc,aAAa;AACpC,QAAI,cAAc,WAAW,IAAI,GAAG;AAClC,YAAM,IAAI;AAAA,QACR,QAAQ,WAAW,IAAI;AAAA,MACzB;AAAA,IACF;AAAA,EACF;AAEA,QAAM,QACJ,OAAO,mBAAmB,aACtB,MAAM,eAAe,EAAE,QAAQ,SAAS,UAAU,CAAC,IACnD;AAEN,MAAI;AACJ,QAAM,eAAe,MAAM,cAAc;AAAA,IACvC,GAAI;AAAA,IACJ,GAAG;AAAA,IAEH;AAAA,IACA;AAAA,IAEA,GAAI,CAAC,CAAC,SAAS,EAAE,OAAO,IAAI;AAAA,IAC5B,UAAU,CAAC,GAAG,QAAQ;AAAA,IACtB,OAAO,cAAc,OAAO,WAA4C;AAAA,IACxE,GAAI,aAAa,EAAE,WAAW,IAAI;AAAA,EACpC,CAAC;AACD,WAAS,aAAa;AAGtB,QAAM,kBAAkB,kBAAkB,YAAY,SAAS;AAC/D,MAAI,iBAAiB;AACnB,aAAS,OAAO,gBAAY,0CAAiB,aAAa,WAAW,CAAC;AAAA,EACxE;AAEA,MAAI,mBAAmB,UAAU;AAE/B,UAAM,OAAO,OAAO,IAAI;AACxB,aAAS,KAAK,CAAC;AACf,QAAI,eAAe,KAAK,CAAC;AAEzB,QAAI,UAAU;AACZ,UAAI;AACJ,qBAAe,aAAa,gBAAY,wCAAgB,CAAC,EAAE;AAAA,QACzD,IAAI,gBAAgB;AAAA,UAClB,UAAU,OAAO;AACf,wBAAY;AAAA,UACd;AAAA,UACA,QAAQ;AACN,gBAAI,CAAC,WAAW,UAAU,UAAU,OAAO,SAAS;AAClD;AAEF,kBAAM,oBAAoB;AAAA,cACxB,GAAG;AAAA,cACH;AAAA,gBACE,MAAM;AAAA,gBACN,SAAS,UAAU;AAAA,cACrB;AAAA,YACF;AACA,qBAAS;AAAA,cACP,UAAU;AAAA,cACV,UAAU;AAAA;AAAA;AAAA,gBAGR,OAAO,UAAU,UAAU;AAAA,cAC7B;AAAA,YACF,CAAC;AAAA,UACH;AAAA,QACF,CAAC;AAAA,MACH;AAAA,IACF;AAGA,iBAAa,OAAO,WAAW,CAAC,EAAE,MAAM,CAAC,MAAM;AAC7C,cAAQ,MAAM,mCAAmC,CAAC;AAAA,IACpD,CAAC;AAAA,EACH;AAEA,SAAO;AACT;AAMO,IAAM,yBAAyB,OACpC,YACG;AACH,QAAM,SAAS,MAAM,qBAAqB,OAAO;AACjD,SAAO,IAAI;AAAA,IACT,OACG,gBAAY,sDAAuB,CAAC,EACpC,gBAAY,wDAAwB,CAAC;AAAA,IACxC;AAAA,MACE,SAAS;AAAA,QACP,gBAAgB;AAAA,MAClB;AAAA,IACF;AAAA,EACF;AACF;AAEO,IAAM,uBAAuB,CAAC,aAA0C;AAAA,EAC7E,MAAM,OAAO,YACX,uBAAuB;AAAA,IACrB,aAAa,QAAQ;AAAA,IACrB,aAAa,MAAM,QAAQ,KAAK;AAAA,IAChC;AAAA,EACF,CAAC;AACL;AAWA,eAAe,cAAc;AAAA,EAC3B;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA,GAAG;AACL,GAAyB;AACvB,SAAO,MAAM,SAAS;AAAA,IACpB,aAAa;AAAA,IACb,MAAM;AAAA,MACJ,MAAM;AAAA,MACN,GAAI,QAAQ,EAAE,MAAM,IAAI;AAAA,MACxB,GAAI,aAAa,EAAE,WAAW,IAAI;AAAA,IACpC;AAAA,IACA,QAAQ,6BAA6B,QAAQ,QAAQ;AAAA,IACrD,GAAI;AAAA,EACN,CAAC;AACH;AAEO,SAAS,6BACd,QACA,UACuB;AACvB,QAAM,wBAA+C,CAAC;AAEtD,MAAI,UAAU,MAAM;AAClB,0BAAsB,KAAK,EAAE,MAAM,UAAU,SAAS,OAAO,CAAC;AAAA,EAChE;AACA,wBAAsB,KAAK,OAAG,wDAAwB,QAAQ,CAAC;AAE/D,SAAO;AACT;","names":[]}