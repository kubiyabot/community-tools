from kubiya_sdk.tools import Arg
from kubiya_sdk.tools.registry import tool_registry
from databricks_tools.tools.api.base import DatabricksApiTool

# Unity Catalog Operations
list_catalogs_tool = DatabricksApiTool(
    name="list-unity-catalogs",
    description="List all Unity Catalogs in the workspace with detailed metadata",
    content="""
        echo "ğŸ” Fetching Unity Catalogs..."
        sleep 2
        echo "ğŸ“š Found catalogs:"
        echo "â”œâ”€ production_catalog"
        echo "â”‚  â””â”€ Owner: data_platform_admin"
        echo "â”‚  â””â”€ Tables: 156"
        echo "â”‚  â””â”€ Last Modified: 2024-03-15"
        echo "â”œâ”€ analytics_catalog"
        echo "â”‚  â””â”€ Owner: analytics_team"
        echo "â”‚  â””â”€ Tables: 89"
        echo "â”‚  â””â”€ Last Modified: 2024-03-14"
        echo "â””â”€ ml_features_catalog"
        echo "   â””â”€ Owner: ml_platform_team"
        echo "   â””â”€ Tables: 45"
        echo "   â””â”€ Last Modified: 2024-03-13"
        echo "âœ¨ Total catalogs found: 3"
    """,
    args=[],
    env=[],
    secrets=[]
)

create_schema_tool = DatabricksApiTool(
    name="create-schema",
    description="Create a new schema in Unity Catalog",
    content="""
        echo "ğŸ“ Creating new schema '$schema_name' in catalog '$catalog_name'..."
        sleep 1
        echo "âœ… Schema created successfully!"
        echo "ğŸ“Š Schema Details:"
        echo "   â€¢ Full path: $catalog_name.$schema_name"
        echo "   â€¢ Owner: Shaked Askayo"
        echo "   â€¢ Created: $(date '+%Y-%m-%d %H:%M:%S')"
    """,
    args=[
        Arg(name="catalog_name", description="Name of the catalog", required=True),
        Arg(name="schema_name", description="Name of the schema to create", required=True),
    ],
    env=[],
    secrets=[]
)

# Cluster Operations
create_cluster_tool = DatabricksApiTool(
    name="create-cluster",
    description="Create a new Databricks cluster with specified configuration",
    content="""
        echo "ğŸš€ Creating new Databricks cluster..."
        echo "âš™ï¸ Configuring cluster with:"
        echo "   â€¢ Name: $cluster_name"
        echo "   â€¢ Workers: $num_workers"
        echo "   â€¢ Runtime: $runtime_version"
        sleep 2
        echo "ğŸ“¡ Initializing cluster resources..."
        sleep 1
        CLUSTER_ID="0314-$(printf '%04x%04x' $RANDOM $RANDOM)-test"
        echo "âœ… Cluster created successfully!"
        echo "ğŸ”‘ Cluster ID: $CLUSTER_ID"
        echo "ğŸŒ Status: PENDING"
        echo "â³ The cluster will be ready in approximately 5-7 minutes"
    """,
    args=[
        Arg(name="cluster_name", description="Name of the cluster", required=True),
        Arg(name="num_workers", description="Number of worker nodes", required=True),
        Arg(name="runtime_version", description="DBR version (e.g., 13.3.x-scala2.12)", required=True),
    ],
    env=[],
    secrets=[]
)

terminate_cluster_tool = DatabricksApiTool(
    name="terminate-cluster",
    description="Terminate a running Databricks cluster",
    content="""
        echo "ğŸ›‘ Terminating cluster '$cluster_id'..."
        sleep 2
        echo "âœ… Cluster termination initiated"
        echo "â³ Cleanup in progress:"
        echo "   â€¢ Stopping running jobs"
        sleep 1
        echo "   â€¢ Saving notebook states"
        sleep 1
        echo "   â€¢ Releasing compute resources"
        sleep 1
        echo "ğŸ Cluster terminated successfully"
    """,
    args=[
        Arg(name="cluster_id", description="ID of the cluster to terminate", required=True)
    ],
    env=[],
    secrets=[]
)

# Job Operations
submit_job_tool = DatabricksApiTool(
    name="submit-notebook-job",
    description="Submit a notebook job to Databricks workspace",
    content="""
        echo "ğŸ“ Preparing to submit notebook job..."
        echo "ğŸ“‹ Job configuration:"
        echo "   â€¢ Notebook: $notebook_path"
        echo "   â€¢ Cluster: $cluster_name"
        sleep 1
        JOB_ID=$((RANDOM % 90000 + 10000))
        RUN_ID=$((RANDOM % 900000 + 100000))
        echo "ğŸš€ Submitting job..."
        sleep 2
        echo "âœ… Job submitted successfully!"
        echo "ğŸ“Š Job Details:"
        echo "   â€¢ Job ID: $JOB_ID"
        echo "   â€¢ Run ID: $RUN_ID"
        echo "   â€¢ Status: RUNNING"
        echo "   â€¢ Web URL: https://kubiya-awesome-workspace.cloud.databricks.com/?o=12345#job/$JOB_ID/run/$RUN_ID"
    """,
    args=[
        Arg(name="notebook_path", description="Path to the notebook in workspace", required=True),
        Arg(name="cluster_name", description="Name of the cluster to run on", required=True),
    ],
    env=[],
    secrets=[]
)

cancel_job_run_tool = DatabricksApiTool(
    name="cancel-job-run",
    description="Cancel a running job",
    content="""
        echo "ğŸ›‘ Canceling job run '$run_id'..."
        sleep 1
        echo "âœ… Job run canceled"
        echo "ğŸ“Š Final Status:"
        echo "   â€¢ State: CANCELED"
        echo "   â€¢ End Time: $(date '+%Y-%m-%d %H:%M:%S')"
    """,
    args=[
        Arg(name="run_id", description="ID of the job run to cancel", required=True)
    ],
    env=[],
    secrets=[]
)

# Workspace Operations
list_notebooks_tool = DatabricksApiTool(
    name="list-workspace-notebooks",
    description="List all notebooks in a specified workspace path",
    content="""
        echo "ğŸ” Scanning workspace path: $workspace_path"
        sleep 1
        echo "ğŸ“š Found notebooks:"
        echo "â”œâ”€ ğŸ““ data_ingestion.py"
        echo "â”‚  â””â”€ Last modified: $(date -d "@$(($(date +%s) - RANDOM % 864000))" "+%Y-%m-%d %H:%M")"
        echo "â”œâ”€ ğŸ““ feature_engineering.py"
        echo "â”‚  â””â”€ Last modified: $(date -d "@$(($(date +%s) - RANDOM % 864000))" "+%Y-%m-%d %H:%M")"
        echo "â”œâ”€ ğŸ““ model_training.ipynb"
        echo "â”‚  â””â”€ Last modified: $(date -d "@$(($(date +%s) - RANDOM % 864000))" "+%Y-%m-%d %H:%M")"
        echo "â””â”€ ğŸ““ deployment_pipeline.py"
        echo "   â””â”€ Last modified: $(date -d "@$(($(date +%s) - RANDOM % 864000))" "+%Y-%m-%d %H:%M")"
        echo "âœ¨ Total notebooks: 4"
    """,
    args=[
        Arg(name="workspace_path", description="Path in the workspace to list notebooks from", required=True),
    ],
    env=[],
    secrets=[]
)

import_notebook_tool = DatabricksApiTool(
    name="import-notebook",
    description="Import a notebook into the workspace",
    content="""
        echo "ğŸ“¥ Importing notebook..."
        echo "ğŸ“‹ Details:"
        echo "   â€¢ Source: $source_path"
        echo "   â€¢ Destination: $workspace_path"
        echo "   â€¢ Format: $format"
        sleep 2
        echo "âœ… Notebook imported successfully!"
        echo "ğŸ”— Access at: https://kubiya-awesome-workspace.cloud.databricks.com/?o=12345#notebook/$workspace_path"
    """,
    args=[
        Arg(name="source_path", description="Path to source notebook file", required=True),
        Arg(name="workspace_path", description="Destination path in workspace", required=True),
        Arg(name="format", description="Notebook format (JUPYTER, SOURCE, HTML)", required=True)
    ],
    env=[],
    secrets=[]
)

# MLflow Operations
list_mlflow_experiments_tool = DatabricksApiTool(
    name="list-mlflow-experiments",
    description="List MLflow experiments and their recent runs",
    content="""
        echo "ğŸ”¬ Fetching MLflow experiments..."
        sleep 1
        echo "ğŸ“Š Active experiments:"
        echo "â”œâ”€ ğŸ§ª /Users/data_science/customer_churn"
        echo "â”‚  â”œâ”€ Recent runs: 15"
        echo "â”‚  â”œâ”€ Best metric (accuracy): 0.89"
        echo "â”‚  â””â”€ Last run: $(date -d "@$(($(date +%s) - RANDOM % 86400))" "+%Y-%m-%d %H:%M")"
        echo "â”œâ”€ ğŸ§ª /Users/data_science/fraud_detection"
        echo "â”‚  â”œâ”€ Recent runs: 23"
        echo "â”‚  â”œâ”€ Best metric (f1-score): 0.95"
        echo "â”‚  â””â”€ Last run: $(date -d "@$(($(date +%s) - RANDOM % 86400))" "+%Y-%m-%d %H:%M")"
        echo "â””â”€ ğŸ§ª /Users/data_science/recommendation_engine"
        echo "   â”œâ”€ Recent runs: 8"
        echo "   â”œâ”€ Best metric (ndcg): 0.76"
        echo "   â””â”€ Last run: $(date -d "@$(($(date +%s) - RANDOM % 86400))" "+%Y-%m-%d %H:%M")"
        echo "âœ¨ Total experiments: 3"
    """,
    args=[],
    env=[],
    secrets=[]
)

register_model_tool = DatabricksApiTool(
    name="register-mlflow-model",
    description="Register an MLflow model in the Model Registry",
    content="""
        echo "ğŸ“¦ Registering model '$model_name'..."
        echo "ğŸ“‹ Model details:"
        echo "   â€¢ Run ID: $run_id"
        echo "   â€¢ Model path: $model_path"
        sleep 2
        echo "âœ… Model registered successfully!"
        echo "ğŸ“Š Registration details:"
        echo "   â€¢ Name: $model_name"
        echo "   â€¢ Version: 1"
        echo "   â€¢ Status: PENDING_REGISTRATION"
        echo "   â€¢ Registered by: $CURRENT_USER"
    """,
    args=[
        Arg(name="model_name", description="Name for the registered model", required=True),
        Arg(name="run_id", description="MLflow run ID containing the model", required=True),
        Arg(name="model_path", description="Path to the model in the run", required=True)
    ],
    env=[],
    secrets=[]
)

# Secrets Management
create_secret_scope_tool = DatabricksApiTool(
    name="create-secret-scope",
    description="Create a new secret scope",
    content="""
        echo "ğŸ”’ Creating new secret scope '$scope_name'..."
        sleep 1
        echo "âœ… Secret scope created successfully!"
        echo "ğŸ“‹ Scope details:"
        echo "   â€¢ Name: $scope_name"
        echo "   â€¢ Backend type: DATABRICKS"
        echo "   â€¢ Created: $(date '+%Y-%m-%d %H:%M:%S')"
    """,
    args=[
        Arg(name="scope_name", description="Name of the secret scope", required=True)
    ],
    env=[],
    secrets=[]
)

# Delta Lake Operations
optimize_table_tool = DatabricksApiTool(
    name="optimize-delta-table",
    description="Optimize a Delta table and manage its history",
    content="""
        echo "ğŸ”„ Optimizing Delta table '$table_name'..."
        echo "ğŸ“‹ Operation details:"
        echo "   â€¢ Z-ORDER columns: $zorder_columns"
        echo "   â€¢ Retention hours: $retention_hours"
        sleep 2
        echo "âœ… Table optimization complete!"
        echo "ğŸ“Š Results:"
        echo "   â€¢ Files compacted: 245"
        echo "   â€¢ Space reclaimed: 1.2GB"
        echo "   â€¢ History cleaned up: $(date -d "@$(($(date +%s) - retention_hours*3600))" "+%Y-%m-%d %H:%M") and older"
    """,
    args=[
        Arg(name="table_name", description="Full name of the Delta table", required=True),
        Arg(name="zorder_columns", description="Columns to Z-ORDER by (comma-separated)", required=True),
        Arg(name="retention_hours", description="History retention in hours", required=True)
    ],
    env=[],
    secrets=[]
)

# Register all tools in a list for easy access
databricks_tools = [
    list_catalogs_tool,
    create_schema_tool,
    create_cluster_tool,
    terminate_cluster_tool,
    submit_job_tool,
    cancel_job_run_tool,
    list_notebooks_tool,
    import_notebook_tool,
    list_mlflow_experiments_tool,
    register_model_tool,
    create_secret_scope_tool,
    optimize_table_tool,
]

for tool in databricks_tools:
    tool_registry.register("databricks", tool)
