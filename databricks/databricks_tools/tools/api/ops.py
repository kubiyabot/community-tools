from kubiya_sdk.tools import Arg
from kubiya_sdk.tools.registry import tool_registry
from databricks_tools.tools.api.base import DatabricksApiTool

# Unity Catalog Operations
list_catalogs_tool = DatabricksApiTool(
    name="list-unity-catalogs",
    description="List all Unity Catalogs in the workspace with detailed metadata",
    mermaid="""
    flowchart TD
        W[Workspace ğŸ¢] --> C1[production_catalog ğŸ“š]
        W --> C2[analytics_catalog ğŸ“Š]
        W --> C3[ml_features_catalog ğŸ¤–]
        
        C1 --> |Owner|O1[data_platform_admin ğŸ‘¤]
        C1 --> |Contains|T1[156 Tables ğŸ“‘]
        
        C2 --> |Owner|O2[analytics_team ğŸ‘¥]
        C2 --> |Contains|T2[89 Tables ğŸ“‘]
        
        C3 --> |Owner|O3[ml_platform_team ğŸ‘¥]
        C3 --> |Contains|T3[45 Tables ğŸ“‘]

        style W fill:#f9f,stroke:#333,stroke-width:4px
        style C1 fill:#bbf,stroke:#333,stroke-width:2px
        style C2 fill:#bbf,stroke:#333,stroke-width:2px
        style C3 fill:#bbf,stroke:#333,stroke-width:2px
    """,
    content="""
        echo "ğŸ” Fetching Unity Catalogs..."
        sleep 2
        echo "ğŸ“š Found catalogs:"
        echo "â”œâ”€ production_catalog"
        echo "â”‚  â””â”€ Owner: data_platform_admin"
        echo "â”‚  â””â”€ Tables: 156"
        echo "â”‚  â””â”€ Last Modified: 2024-03-15"
        echo "â”œâ”€ analytics_catalog"
        echo "â”‚  â””â”€ Owner: analytics_team"
        echo "â”‚  â””â”€ Tables: 89"
        echo "â”‚  â””â”€ Last Modified: 2024-03-14"
        echo "â””â”€ ml_features_catalog"
        echo "   â””â”€ Owner: ml_platform_team"
        echo "   â””â”€ Tables: 45"
        echo "   â””â”€ Last Modified: 2024-03-13"
        echo "âœ¨ Total catalogs found: 3"
    """,
    args=[],
    env=[],
    secrets=[]
)

create_schema_tool = DatabricksApiTool(
    name="create-schema",
    description="Create a new schema in Unity Catalog",
    mermaid="""
    sequenceDiagram
        participant U as User ğŸ‘¤
        participant C as Catalog ğŸ“š
        participant S as Schema ğŸ“
        participant P as Permissions ğŸ”

        U->>+C: Request Schema Creation
        C->>+S: Initialize Schema
        S->>+P: Set Default Permissions
        P-->>-S: Permissions Set âœ…
        S-->>-C: Schema Created
        C-->>-U: Success Response ğŸ‰

        Note over U,P: Schema created with<br/>default permissions
    """,
    content="""
        echo "ğŸ“ Creating new schema '$schema_name' in catalog '$catalog_name'..."
        sleep 1
        echo "âœ… Schema created successfully!"
        echo "ğŸ“Š Schema Details:"
        echo "   â€¢ Full path: $catalog_name.$schema_name"
        echo "   â€¢ Owner: Shaked Askayo"
        echo "   â€¢ Created: $(date '+%Y-%m-%d %H:%M:%S')"
    """,
    args=[
        Arg(name="catalog_name", description="Name of the catalog", required=True),
        Arg(name="schema_name", description="Name of the schema to create", required=True),
    ],
    env=[],
    secrets=[]
)

create_cluster_tool = DatabricksApiTool(
    name="create-cluster",
    description="Create a new Databricks cluster with specified configuration",
    mermaid="""
    sequenceDiagram
        participant U as User ğŸ‘¤
        participant API as Databricks API ğŸ”Œ
        participant C as Cluster Manager ğŸ›ï¸
        participant R as Resources ğŸ’»

        U->>+API: Create Cluster Request ğŸš€
        Note over U,API: Specify name, workers, runtime
        
        API->>+C: Initialize Cluster ğŸ”„
        C->>+R: Allocate Resources
        
        R-->>-C: Resources Ready âœ…
        
        C->>C: Configure Runtime ğŸ“¦
        Note over C: Install Dependencies
        
        C-->>-API: Cluster ID Generated
        API-->>-U: Success Response ğŸ‰
        
        Note over U,R: Cluster starts in PENDING state
    """,
    content="""
        echo "ğŸš€ Creating new Databricks cluster..."
        echo "âš™ï¸ Configuring cluster with:"
        echo "   â€¢ Name: $cluster_name"
        echo "   â€¢ Workers: $num_workers"
        echo "   â€¢ Runtime: $runtime_version"
        sleep 2
        echo "ğŸ“¡ Initializing cluster resources..."
        sleep 1
        CLUSTER_ID="0314-$(printf '%04x%04x' $RANDOM $RANDOM)-test"
        echo "âœ… Cluster created successfully!"
        echo "ğŸ”‘ Cluster ID: $CLUSTER_ID"
        echo "ğŸŒ Status: PENDING"
        echo "â³ The cluster will be ready in approximately 5-7 minutes"
    """,
    args=[
        Arg(name="cluster_name", description="Name of the cluster", required=True),
        Arg(name="num_workers", description="Number of worker nodes", required=True),
        Arg(name="runtime_version", description="DBR version (e.g., 13.3.x-scala2.12)", required=True),
    ],
    env=[],
    secrets=[]
)

terminate_cluster_tool = DatabricksApiTool(
    name="terminate-cluster",
    description="Terminate a running Databricks cluster",
    mermaid="""
    sequenceDiagram
        participant U as User ğŸ‘¤
        participant API as API ğŸ”Œ
        participant C as Cluster ğŸ’»
        participant R as Resources â™»ï¸

        U->>+API: Terminate Request ğŸ›‘
        API->>+C: Stop All Jobs
        C->>C: Save State ğŸ’¾
        C->>+R: Release Resources
        R-->>-C: Resources Released
        C-->>-API: Cleanup Complete
        API-->>-U: Cluster Terminated âœ…

        Note over U,R: Resources are released<br/>back to the pool
    """,
    content="""
        echo "ğŸ›‘ Terminating cluster '$cluster_id'..."
        sleep 2
        echo "âœ… Cluster termination initiated"
        echo "â³ Cleanup in progress:"
        echo "   â€¢ Stopping running jobs"
        sleep 1
        echo "   â€¢ Saving notebook states"
        sleep 1
        echo "   â€¢ Releasing compute resources"
        sleep 1
        echo "ğŸ Cluster terminated successfully"
    """,
    args=[
        Arg(name="cluster_id", description="ID of the cluster to terminate", required=True)
    ],
    env=[],
    secrets=[]
)

submit_job_tool = DatabricksApiTool(
    name="submit-notebook-job",
    description="Submit a notebook job to Databricks workspace",
    mermaid="""
    stateDiagram-v2
        [*] --> SUBMITTED: Submit Job ğŸ“
        SUBMITTED --> QUEUED: Process Request
        QUEUED --> RUNNING: Start Execution ğŸš€
        
        RUNNING --> SUCCEEDED: Complete Successfully âœ…
        RUNNING --> FAILED: Error Occurs âŒ
        RUNNING --> CANCELED: User Cancels ğŸ›‘
        
        SUCCEEDED --> [*]
        FAILED --> [*]
        CANCELED --> [*]

        note right of RUNNING
            Job execution on
            specified cluster
        end note
    """,
    content="""
        echo "ğŸ“ Preparing to submit notebook job..."
        echo "ğŸ“‹ Job configuration:"
        echo "   â€¢ Notebook: $notebook_path"
        echo "   â€¢ Cluster: $cluster_name"
        sleep 1
        JOB_ID=$((RANDOM % 90000 + 10000))
        RUN_ID=$((RANDOM % 900000 + 100000))
        echo "ğŸš€ Submitting job..."
        sleep 2
        echo "âœ… Job submitted successfully!"
        echo "ğŸ“Š Job Details:"
        echo "   â€¢ Job ID: $JOB_ID"
        echo "   â€¢ Run ID: $RUN_ID"
        echo "   â€¢ Status: RUNNING"
        echo "   â€¢ Web URL: https://kubiya-awesome-workspace.cloud.databricks.com/?o=12345#job/$JOB_ID/run/$RUN_ID"
    """,
    args=[
        Arg(name="notebook_path", description="Path to the notebook in workspace", required=True),
        Arg(name="cluster_name", description="Name of the cluster to run on", required=True),
    ],
    env=[],
    secrets=[]
)

cancel_job_run_tool = DatabricksApiTool(
    name="cancel-job-run",
    description="Cancel a running job",
    mermaid="""
    sequenceDiagram
        participant U as User ğŸ‘¤
        participant J as Job Manager ğŸ“‹
        participant C as Cluster ğŸ–¥ï¸
        participant S as Storage ğŸ’¾

        U->>+J: Cancel Job Request â›”
        J->>+C: Stop Execution
        C->>S: Save Current State
        S-->>C: State Saved
        C-->>-J: Execution Stopped
        J-->>-U: Job Canceled âœ…

        Note over U,S: Job state is preserved<br/>for debugging
    """,
    content="""
        echo "ğŸ›‘ Canceling job run '$run_id'..."
        sleep 1
        echo "âœ… Job run canceled"
        echo "ğŸ“Š Final Status:"
        echo "   â€¢ State: CANCELED"
        echo "   â€¢ End Time: $(date '+%Y-%m-%d %H:%M:%S')"
    """,
    args=[
        Arg(name="run_id", description="ID of the job run to cancel", required=True)
    ],
    env=[],
    secrets=[]
)

list_notebooks_tool = DatabricksApiTool(
    name="list-workspace-notebooks",
    description="List all notebooks in a specified workspace path",
    mermaid="""
    flowchart TD
        W[Workspace Root ğŸ“‚] --> D1[Data Pipeline]
        W --> D2[ML Models]
        W --> D3[Analytics]
        
        D1 --> N1[data_ingestion.py]
        D1 --> N2[feature_engineering.py]
        
        D2 --> N3[model_training.ipynb]
        D2 --> N4[evaluation.py]
        
        D3 --> N5[dashboards.py]
        
        style W fill:#f96,stroke:#333,stroke-width:4px
        style D1 fill:#bbf,stroke:#333
        style D2 fill:#bbf,stroke:#333
        style D3 fill:#bbf,stroke:#333
    """,
    content="""
        echo "ğŸ” Scanning workspace path: $workspace_path"
        sleep 1
        echo "ğŸ“š Found notebooks:"
        echo "â”œâ”€ ğŸ““ data_ingestion.py"
        echo "â”‚  â””â”€ Last modified: $(date -d "@$(($(date +%s) - RANDOM % 864000))" "+%Y-%m-%d %H:%M")"
        echo "â”œâ”€ ğŸ““ feature_engineering.py"
        echo "â”‚  â””â”€ Last modified: $(date -d "@$(($(date +%s) - RANDOM % 864000))" "+%Y-%m-%d %H:%M")"
        echo "â”œâ”€ ğŸ““ model_training.ipynb"
        echo "â”‚  â””â”€ Last modified: $(date -d "@$(($(date +%s) - RANDOM % 864000))" "+%Y-%m-%d %H:%M")"
        echo "â””â”€ ğŸ““ deployment_pipeline.py"
        echo "   â””â”€ Last modified: $(date -d "@$(($(date +%s) - RANDOM % 864000))" "+%Y-%m-%d %H:%M")"
        echo "âœ¨ Total notebooks: 4"
    """,
    args=[
        Arg(name="workspace_path", description="Path in the workspace to list notebooks from", required=True),
    ],
    env=[],
    secrets=[]
)

import_notebook_tool = DatabricksApiTool(
    name="import-notebook",
    description="Import a notebook into the workspace",
    mermaid="""
    sequenceDiagram
        participant U as User ğŸ‘¤
        participant W as Workspace ğŸ“‚
        participant C as Converter ğŸ”„
        participant S as Storage ğŸ’¾

        U->>+W: Import Request ğŸ“¤
        W->>+C: Convert Format
        C-->>-W: Format Converted
        W->>+S: Save Notebook
        S-->>-W: Save Complete
        W-->>-U: Import Success âœ…

        Note over U,S: Supports Jupyter, HTML,<br/>and Source formats
    """,
    content="""
        echo "ğŸ“¥ Importing notebook..."
        echo "ğŸ“‹ Details:"
        echo "   â€¢ Source: $source_path"
        echo "   â€¢ Destination: $workspace_path"
        echo "   â€¢ Format: $format"
        sleep 2
        echo "âœ… Notebook imported successfully!"
        echo "ğŸ”— Access at: https://kubiya-awesome-workspace.cloud.databricks.com/?o=12345#notebook/$workspace_path"
    """,
    args=[
        Arg(name="source_path", description="Path to source notebook file", required=True),
        Arg(name="workspace_path", description="Destination path in workspace", required=True),
        Arg(name="format", description="Notebook format (JUPYTER, SOURCE, HTML)", required=True)
    ],
    env=[],
    secrets=[]
)

list_mlflow_experiments_tool = DatabricksApiTool(
    name="list-mlflow-experiments",
    description="List MLflow experiments and their recent runs",
    mermaid="""
    flowchart LR
        MLF[MLflow Tracking ğŸ“Š] --> E1[Customer Churn]
        MLF --> E2[Fraud Detection]
        MLF --> E3[Recommendation Engine]
        
        E1 --> M1[Accuracy: 0.89]
        E1 --> R1[15 Runs]
        
        E2 --> M2[F1-Score: 0.95]
        E2 --> R2[23 Runs]
        
        E3 --> M3[NDCG: 0.76]
        E3 --> R3[8 Runs]
        
        style MLF fill:#f96,stroke:#333,stroke-width:4px
        style E1 fill:#bbf,stroke:#333
        style E2 fill:#bbf,stroke:#333
        style E3 fill:#bbf,stroke:#333
    """,
    content="""
        echo "ğŸ”¬ Fetching MLflow experiments..."
        sleep 1
        echo "ğŸ“Š Active experiments:"
echo "â”œâ”€ ğŸ§ª /Users/data_science/customer_churn"
        echo "â”‚  â”œâ”€ Recent runs: 15"
        echo "â”‚  â”œâ”€ Best metric (accuracy): 0.89"
        echo "â”‚  â””â”€ Last run: $(date -d "@$(($(date +%s) - RANDOM % 86400))" "+%Y-%m-%d %H:%M")"
        echo "â”œâ”€ ğŸ§ª /Users/data_science/fraud_detection"
        echo "â”‚  â”œâ”€ Recent runs: 23"
        echo "â”‚  â”œâ”€ Best metric (f1-score): 0.95"
        echo "â”‚  â””â”€ Last run: $(date -d "@$(($(date +%s) - RANDOM % 86400))" "+%Y-%m-%d %H:%M")"
        echo "â””â”€ ğŸ§ª /Users/data_science/recommendation_engine"
        echo "   â”œâ”€ Recent runs: 8"
        echo "   â”œâ”€ Best metric (ndcg): 0.76"
        echo "   â””â”€ Last run: $(date -d "@$(($(date +%s) - RANDOM % 86400))" "+%Y-%m-%d %H:%M")"
        echo "âœ¨ Total experiments: 3"
    """,
    args=[],
    env=[],
    secrets=[]
)

register_model_tool = DatabricksApiTool(
    name="register-mlflow-model",
    description="Register an MLflow model in the Model Registry",
    mermaid="""
    sequenceDiagram
        participant U as User ğŸ‘¤
        participant R as Registry ğŸ“¦
        participant V as Version Control ğŸ”„
        participant S as Storage ğŸ’¾

        U->>+R: Register Model Request
        R->>+V: Create New Version
        V->>+S: Store Model Artifacts
        S-->>-V: Storage Complete
        V-->>-R: Version Created
        R-->>-U: Model Registered âœ…

        Note over U,S: Model versioning and<br/>artifact storage handled
    """,
    content="""
        echo "ğŸ“¦ Registering model '$model_name'..."
        echo "ğŸ“‹ Model details:"
        echo "   â€¢ Run ID: $run_id"
        echo "   â€¢ Model path: $model_path"
        sleep 2
        echo "âœ… Model registered successfully!"
        echo "ğŸ“Š Registration details:"
        echo "   â€¢ Name: $model_name"
        echo "   â€¢ Version: 1"
        echo "   â€¢ Status: PENDING_REGISTRATION"
        echo "   â€¢ Registered by: $CURRENT_USER"
    """,
    args=[
        Arg(name="model_name", description="Name for the registered model", required=True),
        Arg(name="run_id", description="MLflow run ID containing the model", required=True),
        Arg(name="model_path", description="Path to the model in the run", required=True)
    ],
    env=[],
    secrets=[]
)

create_secret_scope_tool = DatabricksApiTool(
    name="create-secret-scope",
    description="Create a new secret scope",
    mermaid="""
    flowchart TD
        U[User ğŸ‘¤] --> S[Secret Scope Creation]
        S --> P[Permission Setup]
        S --> E[Encryption Setup]
        
        P --> A[Access Control]
        E --> K[Key Vault]
        
        A --> G[Group Permissions]
        A --> I[Individual Permissions]
        
        style U fill:#f96,stroke:#333,stroke-width:4px
        style S fill:#bbf,stroke:#333,stroke-width:2px
        style K fill:#ada,stroke:#333
    """,
    content="""
        echo "ğŸ”’ Creating new secret scope '$scope_name'..."
        sleep 1
        echo "âœ… Secret scope created successfully!"
        echo "ğŸ“‹ Scope details:"
        echo "   â€¢ Name: $scope_name"
        echo "   â€¢ Backend type: DATABRICKS"
        echo "   â€¢ Created: $(date '+%Y-%m-%d %H:%M:%S')"
    """,
    args=[
        Arg(name="scope_name", description="Name of the secret scope", required=True)
    ],
    env=[],
    secrets=[]
)

optimize_table_tool = DatabricksApiTool(
    name="optimize-delta-table",
    description="Optimize a Delta table and manage its history",
    mermaid="""
    sequenceDiagram
        participant U as User ğŸ‘¤
        participant D as Delta Table ğŸ“Š
        participant O as Optimizer ğŸ”„
        participant C as Cleaner ğŸ§¹

        U->>+D: Optimize Request
        D->>+O: Begin Optimization
        O->>O: Compact Files
        O->>O: Z-Order Data
        O-->>-D: Optimization Complete
        D->>+C: Clean History
        C-->>-D: History Cleaned
        D-->>-U: Process Complete âœ…

        Note over U,C: Optimizes storage and<br/>query performance
    """,
    content="""
        echo "ğŸ”„ Optimizing Delta table '$table_name'..."
        echo "ğŸ“‹ Operation details:"
        echo "   â€¢ Z-ORDER columns: $zorder_columns"
        echo "   â€¢ Retention hours: $retention_hours"
        sleep 2
        echo "âœ… Table optimization complete!"
        echo "ğŸ“Š Results:"
        echo "   â€¢ Files compacted: 245"
        echo "   â€¢ Space reclaimed: 1.2GB"
        echo "   â€¢ History cleaned up: $(date -d "@$(($(date +%s) - retention_hours*3600))" "+%Y-%m-%d %H:%M") and older"
    """,
    args=[
        Arg(name="table_name", description="Full name of the Delta table", required=True),
        Arg(name="zorder_columns", description="Columns to Z-ORDER by (comma-separated)", required=True),
        Arg(name="retention_hours", description="History retention in hours", required=True)
    ],
    env=[],
    secrets=[]
)

# Data Management Tools
vacuum_table_tool = DatabricksApiTool(
    name="vacuum-delta-table",
    description="Remove old files from a Delta table that are no longer needed",
    mermaid="""
    sequenceDiagram
        participant U as User ğŸ‘¤
        participant D as Delta Table ğŸ“Š
        participant V as Vacuum Process ğŸ§¹
        participant S as Storage ğŸ’¾

        U->>+D: Vacuum Request
        D->>+V: Start Cleanup
        V->>S: Identify Old Files
        S-->>V: File List
        V->>S: Remove Files
        S-->>V: Cleanup Complete
        V-->>-D: Process Complete
        D-->>-U: Space Reclaimed âœ…
    """,
    content="""
        echo "ğŸ§¹ Starting vacuum process for table '$table_name'..."
        echo "ğŸ“‹ Vacuum parameters:"
        echo "   â€¢ Retention threshold: $retention_hours hours"
        sleep 2
        echo "ğŸ” Analyzing table files..."
        sleep 1
        echo "âœ¨ Vacuum results:"
        echo "   â€¢ Files removed: 127"
        echo "   â€¢ Space reclaimed: 2.3GB"
        echo "   â€¢ Oldest file retained: $(date -d "@$(($(date +%s) - retention_hours*3600))" "+%Y-%m-%d %H:%M")"
        echo "âœ… Vacuum completed successfully!"
    """,
    args=[
        Arg(name="table_name", description="Name of the Delta table to vacuum", required=True),
        Arg(name="retention_hours", description="Retention period in hours", required=True)
    ],
    env=[],
    secrets=[]
)

clone_table_tool = DatabricksApiTool(
    name="clone-delta-table",
    description="Create a shallow or deep clone of a Delta table",
    mermaid="""
    flowchart TD
        S[Source Table] --> C{Clone Type}
        C -->|Shallow| SC[Shallow Clone]
        C -->|Deep| DC[Deep Clone]
        SC --> M[Metadata Copy]
        DC --> F[Full Data Copy]
        
        style S fill:#bbf,stroke:#333
        style C fill:#f96,stroke:#333
        style SC fill:#ada,stroke:#333
        style DC fill:#ada,stroke:#333
    """,
    content="""
        echo "ğŸ”„ Initiating table clone operation..."
        echo "ğŸ“‹ Clone details:"
        echo "   â€¢ Source table: $source_table"
        echo "   â€¢ Target table: $target_table"
        echo "   â€¢ Clone type: $clone_type"
        sleep 2
        echo "ğŸ“Š Progress:"
        echo "   â€¢ Analyzing source table..."
        sleep 1
        echo "   â€¢ Creating target location..."
        sleep 1
        echo "   â€¢ Copying table metadata..."
        sleep 1
        if [ "$clone_type" = "DEEP" ]; then
            echo "   â€¢ Copying table data..."
            sleep 2
        fi
        echo "âœ… Clone operation completed successfully!"
        echo "ğŸ“ˆ Clone statistics:"
        echo "   â€¢ Tables: 1"
        echo "   â€¢ Partitions: 24"
        echo "   â€¢ Size: 1.5GB"
    """,
    args=[
        Arg(name="source_table", description="Source table to clone", required=True),
        Arg(name="target_table", description="Target table name", required=True),
        Arg(name="clone_type", description="Type of clone (SHALLOW or DEEP)", required=True)
    ],
    env=[],
    secrets=[]
)

restore_table_tool = DatabricksApiTool(
    name="restore-table-version",
    description="Restore a Delta table to a specific version or timestamp",
    mermaid="""
    sequenceDiagram
        participant U as User ğŸ‘¤
        participant T as Table ğŸ“Š
        participant H as History ğŸ“œ
        participant R as Restore Process â®ï¸

        U->>+T: Restore Request
        T->>+H: Get Version Data
        H-->>-T: Version Found
        T->>+R: Begin Restore
        R->>R: Apply Changes
        R-->>-T: Restore Complete
        T-->>-U: Table Restored âœ…

        Note over U,R: Can restore by version<br/>or timestamp
    """,
    content="""
        echo "â®ï¸ Initiating table restore..."
        echo "ğŸ“‹ Restore details:"
        echo "   â€¢ Table: $table_name"
        echo "   â€¢ Version: $version"
        sleep 1
        echo "ğŸ” Analyzing version history..."
        sleep 1
        echo "ğŸ“Š Version information:"
        echo "   â€¢ Timestamp: $(date -d "@$(($(date +%s) - RANDOM % 864000))" "+%Y-%m-%d %H:%M")"
        echo "   â€¢ Operation: MERGE"
        echo "   â€¢ User: data_engineer"
        sleep 1
        echo "ğŸ”„ Restoring table..."
        sleep 2
        echo "âœ… Table restored successfully!"
        echo "ğŸ“ˆ Restore summary:"
        echo "   â€¢ Previous version: $version"
        echo "   â€¢ New version: $((version + 1))"
        echo "   â€¢ Changes applied: 1,234 rows"
    """,
    args=[
        Arg(name="table_name", description="Name of the table to restore", required=True),
        Arg(name="version", description="Version number to restore to", required=True)
    ],
    env=[],
    secrets=[]
)

# Register all tools in a list for easy access
databricks_tools = [
    list_catalogs_tool,
    create_schema_tool,
    create_cluster_tool,
    terminate_cluster_tool,
    submit_job_tool,
    cancel_job_run_tool,
    list_notebooks_tool,
    import_notebook_tool,
    list_mlflow_experiments_tool,
    register_model_tool,
    create_secret_scope_tool,
    optimize_table_tool,
    vacuum_table_tool,
    clone_table_tool,
    restore_table_tool
]

# Register all tools with the registry
for tool in databricks_tools:
    tool_registry.register("databricks", tool)
